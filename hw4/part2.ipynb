{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uQQwn7SH7w0r"
   },
   "source": [
    "# EECS 442 Homework 4 - PyTorch ConvNets\n",
    "In this notebook we will explore how to use a pre-trained PyTorch convolution neural network (ConvNet).\n",
    "\n",
    "Before we start, please put your name and UMID in following format\n",
    "\n",
    ": Firstname LASTNAME, #00000000   //   e.g.) David FOUHEY, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eFPFu-q83RT"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hello EECS442 #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avzyZRQ4HXEQ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGyzNtrV9Gtt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "SQUEEZENET_MEAN = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float)\n",
    "SQUEEZENET_STD = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float)\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JW4zzv00YcT6",
    "outputId": "6fa57f41-214c-491f-b497-9c6e88dae8ff"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU. You are good to go!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"Using the CPU. Overall speed may be slowed down\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6V0Z5Lw_cuo"
   },
   "source": [
    "For all of our experiments, we will start with a convolutional neural network which was pretrained to perform image classification on ImageNet [1]. We can use any model here, but for the purposes of this assignment we will use SqueezeNet [2], which achieves accuracies comparable to AlexNet but with a significantly reduced parameter count and computational complexity.\n",
    "\n",
    "Using SqueezeNet rather than AlexNet or VGG or ResNet means that we can easily perform all experiments without heavy computation. Run the following cell to download and initialize your model.\n",
    "\n",
    "[1] Olga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015\n",
    "\n",
    "[2] Iandola et al, \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size\", arXiv 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "238bb82737e24d228f9c2a2841b4c340",
      "06bcca7c190040aca7181e42901f53f9",
      "b5247475c08248589b31cbe430a0d24e",
      "8753a3852ac44549b3a962c06ef87918",
      "bc631306b00c4e7db3facc98a49db2d0",
      "cfd65b0286ec4a739e6c1197fa972c00",
      "06399db82b7c4f71b3a5b18a38675555",
      "f3cfcf0a554548caa2c388bdae12e473"
     ]
    },
    "id": "JwcF2163_d9X",
    "outputId": "f3bb3379-efd8-490b-e96a-5d0246de136d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Download and load the pretrained SqueezeNet model.')\n",
    "model = torchvision.models.squeezenet1_1(pretrained=True).to(device)\n",
    "\n",
    "# We don't want to train the model, so tell PyTorch not to compute gradients\n",
    "# with respect to model parameters.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Make sure the model is in \"eval\" mode\n",
    "model.eval()\n",
    "\n",
    "# you may see warning regarding initialization deprecated, that's fine, \n",
    "# please continue to next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meTruYDBGSLY"
   },
   "source": [
    "Next, we will download the imagenet labels which we are going to use in the notebook. ImageNet labels are stored in the `idx2label` dictionary of `{index(int): label(str)}`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHL95LvKEhFx",
    "outputId": "969b6bae-7ee5-4664-df4d-d305ba4e0f83",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the imagenet class labels\n",
    "# If this cell failed due to wget probelm, you can put the link below into your browser to download the file directly\n",
    "# Put the downloaded file under the same directory as this jupyter notebook\n",
    "!wget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
    "\n",
    "class_idx = json.load(open(\"imagenet_class_index.json\"))\n",
    "idx2label = {k:class_idx[str(k)][1] for k in range(len(class_idx))}\n",
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAGOycoQ_X8x"
   },
   "source": [
    "### Helper Functions\n",
    "\n",
    "Our pretrained model was trained on images that had been preprocessed by subtracting the per-color mean and dividing by the per-color standard deviation. We define a few helper functions for performing and undoing this preprocessing. You don't need to do anything in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgrlY4_U_Z1S"
   },
   "outputs": [],
   "source": [
    "def preprocess(img, size=224):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((size, size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n",
    "              std=SQUEEZENET_STD.tolist()),\n",
    "        T.Lambda(lambda x: x[None]),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def deprocess(img, should_rescale=True):\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda x: x[0]),\n",
    "        T.Normalize(mean=[0, 0, 0], std=(1.0 / SQUEEZENET_STD).tolist()),\n",
    "        T.Normalize(mean=(-SQUEEZENET_MEAN).tolist(), std=[1, 1, 1]),\n",
    "        T.Lambda(rescale) if should_rescale else T.Lambda(lambda x: x),\n",
    "        T.ToPILImage(),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def rescale(x):\n",
    "    low, high = x.min(), x.max()\n",
    "    x_rescaled = (x - low) / (high - low)\n",
    "    return x_rescaled\n",
    "  \n",
    "def blur_image(X, sigma=1):\n",
    "    X_np = X.cpu().clone().numpy()\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=2)\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=3)\n",
    "    X.copy_(torch.Tensor(X_np).type_as(X))\n",
    "    return X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nrrlCIufBt3e"
   },
   "source": [
    "## Task 6 - Pre-trained Convolution Network\n",
    "In order to get a better sense of the classification decisions made by convolutional networks, your job is now to experiment by running whatever images you want through a model pretrained on ImageNet. These can be images from your own photo collection, from the internet, or somewhere else but they **should belong to one of the ImageNet classes**. Look at the `idx2label` dictionary for all the ImageNetclasses.\n",
    "\n",
    "You need to find:\n",
    "1. One image (`img1`) where the SqueezeNet model gives reasonable predictions, and produces a category label that seems to correctly describe the content of the image\n",
    "2. One image (`img2`) where the SqueezeNet model gives unreasonable predictions, and produces a category label that does not correctly describe the content of the image.\n",
    "\n",
    "You can upload images in Colab by using the upload button on the top left. For more details about using Colab, please see our [Colab tutorial](https://web.eecs.umich.edu/~justincj/teaching/eecs442/WI2021/colab.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "xHuRe4VXBtf0",
    "outputId": "f3705ae6-179c-45e5-e639-d704876263e3"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# TODO: Upload your image and run the forward pass to get the ImageNet class. #\n",
    "# This code will crash when you run it, since the maxresdefault.jpg image is  #\n",
    "# not found. You should upload your own images to the Colab notebook and edit #\n",
    "# these lines to load your own image.                                         #\n",
    "###############################################################################\n",
    "img1 = None\n",
    "img2 = None\n",
    "names = ['image1 name', 'image2 name']\n",
    "##############################################################################\n",
    "#               END OF YOUR CODE                                             #\n",
    "##############################################################################\n",
    "for i, img in enumerate([img1, img2]):\n",
    "    X = preprocess(img).to(device)\n",
    "    pred_class = torch.argmax(model(X)).item()\n",
    "    plt.figure(figsize=(6,8))\n",
    "    plt.imshow(img)\n",
    "    plt.title('Predicted Class: %s' % idx2label[pred_class])\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{names[i]}_pred.jpg')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "part2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9d34995e9b9aacf7236904ea7ddf7585a36f6be47e873dd30641939c3248d078"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06399db82b7c4f71b3a5b18a38675555": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "06bcca7c190040aca7181e42901f53f9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "238bb82737e24d228f9c2a2841b4c340": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b5247475c08248589b31cbe430a0d24e",
       "IPY_MODEL_8753a3852ac44549b3a962c06ef87918"
      ],
      "layout": "IPY_MODEL_06bcca7c190040aca7181e42901f53f9"
     }
    },
    "8753a3852ac44549b3a962c06ef87918": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3cfcf0a554548caa2c388bdae12e473",
      "placeholder": "​",
      "style": "IPY_MODEL_06399db82b7c4f71b3a5b18a38675555",
      "value": " 4.74M/4.74M [01:00&lt;00:00, 82.5kB/s]"
     }
    },
    "b5247475c08248589b31cbe430a0d24e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfd65b0286ec4a739e6c1197fa972c00",
      "max": 4966400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bc631306b00c4e7db3facc98a49db2d0",
      "value": 4966400
     }
    },
    "bc631306b00c4e7db3facc98a49db2d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cfd65b0286ec4a739e6c1197fa972c00": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3cfcf0a554548caa2c388bdae12e473": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
